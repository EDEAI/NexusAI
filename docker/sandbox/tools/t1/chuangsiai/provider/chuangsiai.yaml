credentials_for_provider:
  chuangsi_access_key:
    help:
      en_US: Visit the Chuangsi LLM Safety Console to get your Access Key
      zh_Hans: 请前往创思大模型安全控制台获取 AccessKey
    label:
      en_US: Chuangsi LLM Safety Access Key
      zh_Hans: 创思大模型安全 AccessKey
    placeholder:
      en_US: Enter your Chuangsi LLM Safety Access Key
      zh_Hans: 请输入你的创思大模型安全AccessKey
    required: true
    type: text-input
    url: https://console.chuangsiai.com/#/profile/accessKey
  chuangsi_secret_key:
    help:
      en_US: Visit the Chuangsi LLM Safety Console to get your Secret Key
      zh_Hans: 请前往创思大模型安全控制台获取 SecretKey
    label:
      en_US: Chuangsi LLM Safety Secret Key
      zh_Hans: 创思大模型安全 SecretKey
    placeholder:
      en_US: Enter your Chuangsi LLM Safety Secret Key
      zh_Hans: 请输入你的创思大模型安全SecretKey
    required: true
    type: secret-input
    url: https://console.chuangsiai.com/#/profile/accessKey
extra:
  python:
    source: provider/chuangsiai.py
identity:
  author: nexusai
  description:
    en_US: The Chuangsi LLM Safety Toolkit is a comprehensive content security solution
      designed for large language models (LLMs). It is engineered to detect and mitigate
      potential safety risks in both model inputs and outputs, including politically
      sensitive content, explicit material, illegal or non-compliant information,
      harmful value orientation, abusive language, and privacy violations. The tool
      supports safe and trustworthy large language model capabilities to provide secure
      fallback responses for user inputs with security risks, assisting users in ensuring
      the safety and compliance of their LLM systems.
    zh_Hans: 创思大模型安全工具是一款面向大语言模型的内容安全防护系统，旨在识别和拦截LLM输入与输出中的潜在安全风险，包括涉政敏感、色情、违法违规行为、负面价值导向、辱骂言论及隐私泄露等内容。针对存在风险的输入，该工具还支持安全可信代答，助力用户实现大模型系统的安全合规。
  icon: icon.svg
  label:
    en_US: Chuangsi LLM Safety
    zh_Hans: 创思大模型安全
  name: chuangsiai
tools:
- tools/llm_input_safety_guardrails.yaml
- tools/llm_output_safety_guardrails.yaml
